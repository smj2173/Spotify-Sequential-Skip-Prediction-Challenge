# -*- coding: utf-8 -*-
"""ChiSquareTests.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4-xeeaY6y95Dm6OtaN4AowbvmmEQuDI
"""

#Downloads Training Set
#runtime about 3 minutes
!wget https://os.zhdk.cloud.switch.ch/swift/v1/crowdai-public/spotify-sequential-skip-prediction-challenge/split-files/training_set_0.tar.gz

!tar -xvf 'training_set_0.tar.gz'
#Untars the dataset 
#runtime about 4 minutes

import pandas as pd 
import os
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, chi2_contingency
dir = '/content/training_set/'

skip1_context_switch, skip2_context_switch, skip3_context_switch = {}, {}, {}
skip1_nopause, skip2_nopause, skip3_nopause = {}, {}, {}
skip1_prem, skip2_prem, skip3_prem = {}, {}, {}
skip1_hour, skip2_hour, skip3_hour = {}, {}, {}
skip1_ses_pos, skip2_ses_pos, skip3_ses_pos = {}, {}, {}
skip1_longpause, skip2_longpause, skip3_longpause = {}, {}, {}

def find_correlations(file_name, data):
  #premium
  prem_1= pd.crosstab(data["skip_1"], data["premium"])
  skip1_prem[file_name] = chi2_contingency(prem_1)[0:3]
  print(skip1_prem[file_name][0])
  prem_2 = pd.crosstab(data["skip_2"], data["premium"])
  skip2_prem[file_name] = chi2_contingency(prem_2)[0:3]
  prem_3 = pd.crosstab(data["skip_3"], data["premium"])
  skip3_prem[file_name] = chi2_contingency(prem_3)[0:3]

  #context_switch
  cs_1= pd.crosstab(data["skip_1"], data["context_switch"])
  skip1_context_switch[file_name] = chi2_contingency(cs_1)[0:3]
  cs_2= pd.crosstab(data["skip_2"], data["context_switch"])
  skip2_context_switch[file_name] = chi2_contingency(cs_2)[0:3]
  cs_3= pd.crosstab(data["skip_3"], data["context_switch"])
  skip3_context_switch[file_name] = chi2_contingency(cs_3)[0:3]

  #skip1_nopause
  np_1 = pd.crosstab(data["skip_1"], data["no_pause_before_play"])
  skip1_nopause[file_name] = chi2_contingency(np_1)[0:3]
  np_2 = pd.crosstab(data["skip_2"], data["no_pause_before_play"])
  skip2_nopause[file_name] = chi2_contingency(np_2)[0:3]
  np_3 = pd.crosstab(data["skip_3"], data["no_pause_before_play"])
  skip3_nopause[file_name] = chi2_contingency(np_3)[0:3]

  #long pause
  lp_1 = pd.crosstab(data["skip_1"], data["long_pause_before_play"])
  skip1_longpause[file_name] = chi2_contingency(lp_1)[0:3]
  lp_2 = pd.crosstab(data["skip_2"], data["long_pause_before_play"])
  skip2_longpause[file_name] = chi2_contingency(lp_2)[0:3]
  lp_3 = pd.crosstab(data["skip_3"], data["long_pause_before_play"])
  skip3_longpause[file_name] = chi2_contingency(lp_3)[0:3]

  #hour
  h1 = pd.crosstab(data["skip_1"], data["hour_of_day"])
  skip1_hour[file_name] = chi2_contingency(h1)[0:3]
  h2 = pd.crosstab(data["skip_2"], data["hour_of_day"])
  skip2_hour[file_name] = chi2_contingency(h2)[0:3]
  h3 = pd.crosstab(data["skip_3"], data["hour_of_day"])
  skip3_hour[file_name] = chi2_contingency(h3)[0:3]

  #session position
  s1 = pd.crosstab(data["skip_1"], data["session_position"])
  skip1_ses_pos[file_name] = chi2_contingency(s1)[0:3]
  s2 = pd.crosstab(data["skip_2"], data["session_position"])
  skip2_ses_pos[file_name] = chi2_contingency(s2)[0:3]
  s3 = pd.crosstab(data["skip_3"], data["session_position"])
  skip3_ses_pos[file_name] = chi2_contingency(s3)[0:3]


for file in os.listdir(dir):
  if file.endswith(".csv"):
    find_correlations(file, pd.read_csv('/content/training_set/'+file))


temp_context = [list(ele) for ele in skip1_context_switch.values()]
print(temp_context)
mean_skip1_context = sum(i[0] for i in temp_context) / len(skip1_context_switch)
print("mean value correlation between skip1 and context_switch %.3f" % mean_skip1_context)
p_1 = sum(i[1] for i in temp_context) / len(skip1_context_switch)
print("mean p-value %.3f" % p_1)

temp_context2 = [list(ele) for ele in skip2_context_switch.values()]
mean_skip2_context = sum(i[0] for i in temp_context2) / len(skip2_context_switch)
print("mean value between skip2 and context_switch %.3f" % mean_skip2_context)
p_2 = sum(i[1] for i in temp_context2) / len(skip2_context_switch)
print("mean p-value %.3f" % p_2)

temp_context3 = [list(ele) for ele in skip3_context_switch.values()]
mean_skip3_context = sum(i[0] for i in temp_context3) / len(skip3_context_switch)
print("mean value between skip3 and context_switch %.3f" % mean_skip3_context)
p_3 = sum(i[1] for i in temp_context3) / len(skip3_context_switch)
print("mean p-value %.3f" % p_3)

#no_pause_before_play
temp_no1 = [list(ele) for ele in skip1_nopause.values()]
mean_skip1_no_pause = sum(i[0] for i in temp_no1) / len(skip1_nopause)
print("mean value between skip1 and no_pause_before_play: %.3f" % mean_skip1_no_pause)
p_1_2 = sum(i[1] for i in temp_no1) / len(skip1_nopause)
print("mean p-value %.3f" % p_1_2)

temp_no2 = [list(ele) for ele in skip2_nopause.values()]
mean_skip2_no_pause = sum(i[0] for i in temp_no2) / len(skip2_nopause)
print("mean value between skip2 and no_pause_before_play: %.3f" % mean_skip2_no_pause)
p_2_2 = sum(i[1] for i in temp_no2) / len(skip2_nopause)
print("mean p-value %.3f" % p_2_2)

temp_no3 = [list(ele) for ele in skip3_nopause.values()]
mean_skip3_no_pause = sum(i[0] for i in temp_no3) / len(skip3_nopause)
print("mean value between skip3 and no_pause_before_play: %.3f" % mean_skip3_no_pause)
p_3_2 = sum(i[1] for i in temp_no3) / len(skip3_nopause)
print("mean p-value %.3f" % p_3_2)

#premium
temp_prem = [list(ele) for ele in skip1_prem.values()]
mean_skip1_prem = sum(i[0] for i in temp_prem) / len(skip1_prem)
print("mean value between skip1 and premium %.3f" % mean_skip1_prem)
p_1_3 = sum(i[1] for i in temp_prem) / len(skip1_prem)
print("mean p-value %.3f" % p_1_3)

temp_prem2 = [list(ele) for ele in skip2_prem.values()]
mean_skip2_prem = sum(i[0] for i in temp_prem2) / len(skip2_prem)
print("mean value between skip2 and premium %.3f" % mean_skip2_prem)
p_2_3 = sum(i[1] for i in temp_prem2) / len(skip2_prem)
print("mean p-value %.3f" % p_1_2)

temp_prem3 = [list(ele) for ele in skip3_prem.values()]
mean_skip3_prem = sum(i[0] for i in temp_prem3) / len(skip3_prem)
print("mean value between skip3 and premium %.3f" % mean_skip3_prem)
p_3_3 = sum(i[1] for i in temp_prem3) / len(skip3_prem)
print("mean p-value %.3f" % p_3_2)

#hour_of_day
temp_hour = [list(ele) for ele in skip1_hour.values()]
mean_skip1_hour = sum(i[0] for i in temp_hour) / len(skip1_hour)
print("mean value between skip1 and hour of day %.3f" % mean_skip1_hour)
p_1_4 = sum(i[1] for i in temp_hour) / len(skip1_hour)
print("mean p-value %.3f" % p_1_4)

temp_hour2 = [list(ele) for ele in skip2_hour.values()]
mean_skip2_hour = sum(i[0] for i in temp_hour2) / len(skip2_hour)
print("mean value between skip2 and hour of day %.3f" % mean_skip2_hour)
p_2_4 = sum(i[1] for i in temp_hour2) / len(skip2_hour)
print("mean p-value %.3f" % p_2_4)

temp_hour3 = [list(ele) for ele in skip3_hour.values()]
mean_skip3_hour = sum(i[0] for i in temp_hour3) / len(skip3_hour)
print("mean value between skip3 and hour of day %.3f" % mean_skip3_hour)
p_3_4 = sum(i[1] for i in temp_hour3) / len(skip3_hour)
print("mean p-value %.3f" % p_3_4)

#session_position
temp_ses = [list(ele) for ele in skip1_ses_pos.values()]
mean_skip1_ses_pos = sum(i[0] for i in temp_ses) / len(skip1_ses_pos)
print("mean value between skip1 and session position %.3f" % mean_skip1_ses_pos)
p_1_5 = sum(i[1] for i in temp_ses) / len(skip1_ses_pos)
print("mean p-value %.3f" % p_1_5)

temp_ses2 = [list(ele) for ele in skip2_ses_pos.values()]
mean_skip2_ses_pos = sum(i[0] for i in temp_ses2) / len(skip2_ses_pos)
print("mean value between skip2 and session position %.3f" % mean_skip2_ses_pos)
p_2_5 = sum(i[1] for i in temp_ses2) / len(skip2_ses_pos)
print("mean p-value %.3f" % p_2_5)

temp_ses3 = [list(ele) for ele in skip3_ses_pos.values()]
mean_skip3_ses_pos = sum(i[0] for i in temp_ses3) / len(skip3_ses_pos)
print("mean value between skip3 and session position %.3f" % mean_skip3_ses_pos)
p_3_5 = sum(i[1] for i in temp_ses3) / len(skip3_ses_pos)
print("mean p-value %.3f" % p_3_5)

#long_pause_before_play
temp_long = [list(ele) for ele in skip1_longpause.values()]
mean_skip1_longpause = sum(i[0] for i in temp_long) / len(skip1_longpause)
print("mean value between skip1 and long pause before play %.3f" % mean_skip1_longpause)
p_1_6 = sum(i[1] for i in temp_long) / len(skip1_longpause)
print("mean p-value %.3f" % p_1_6)

temp_long2 = [list(ele) for ele in skip2_longpause.values()]
mean_skip2_longpause = sum(i[0] for i in temp_long2) / len(skip2_longpause)
print("mean value between skip2 and long pause before play %.3f" % mean_skip2_longpause)
p_2_6 = sum(i[1] for i in temp_long2) / len(skip2_longpause)
print("mean p-value %.3f" % p_2_6)

temp_long3 = [list(ele) for ele in skip3_longpause.values()]
mean_skip3_longpause = sum(i[0] for i in temp_long3) / len(skip2_longpause)
print("mean value between skip3 and long pause before play %.3f" % mean_skip3_longpause)
p_3_6 = sum(i[1] for i in temp_long3) / len(skip3_longpause)
print("mean p-value %.3f" % p_3_6)

#runtime = about 15 minutes
import pandas as pd 
import os
from scipy.stats import pearsonr
dir = '/content/training_set/'

skip1_length, skip2_length, skip3_length = {}, {}, {}
skip1_seekfwd, skip2_seekfwd, skip3_seekfwd = {}, {}, {}
skip1_seekback, skip2_seekback, skip3_seekback = {}, {}, {}
skip1_shuffle, skip2_shuffle, skip3_shuffle = {}, {}, {}
skip1_context, skip2_context, skip3_context = {}, {}, {}

def find_more_correlations(file_name, data):

  #session length
  l1 = pd.crosstab(data["skip_1"], data["session_length"])
  skip1_length[file_name] = chi2_contingency(l1)[0:3]
  l2 = pd.crosstab(data["skip_2"], data["session_length"])
  skip2_length[file_name] = chi2_contingency(l2)[0:3]
  l3 = pd.crosstab(data["skip_3"], data["session_length"])
  skip3_length[file_name] = chi2_contingency(l3)[0:3]

  #seekfwd
  sf1= pd.crosstab(data["skip_1"], data["hist_user_behavior_n_seekfwd"])
  skip1_seekfwd[file_name] = chi2_contingency(sf1)[0:3]
  sf2= pd.crosstab(data["skip_2"], data["hist_user_behavior_n_seekfwd"])
  skip2_seekfwd[file_name] = chi2_contingency(sf2)[0:3]
  sf3= pd.crosstab(data["skip_3"], data["hist_user_behavior_n_seekfwd"])
  skip3_seekfwd[file_name] = chi2_contingency(sf3)[0:3]

  #seekback
  sb1 = pd.crosstab(data["skip_1"], data["hist_user_behavior_n_seekback"])
  skip1_seekback[file_name] = chi2_contingency(sb1)[0:3]
  sb2 = pd.crosstab(data["skip_2"], data["hist_user_behavior_n_seekback"])
  skip2_seekback[file_name] = chi2_contingency(sb2)[0:3]
  sb3 = pd.crosstab(data["skip_3"], data["hist_user_behavior_n_seekback"])
  skip3_seekback[file_name] = chi2_contingency(sb3)[0:3]

  #shuffle
  s1 = pd.crosstab(data["skip_1"], data["hist_user_behavior_is_shuffle"])
  skip1_shuffle[file_name] = chi2_contingency(s1)[0:3]
  s2 = pd.crosstab(data["skip_2"], data["hist_user_behavior_is_shuffle"])
  skip2_shuffle[file_name] = chi2_contingency(s2)[0:3]
  s3 = pd.crosstab(data["skip_3"], data["hist_user_behavior_is_shuffle"])
  skip3_shuffle[file_name] = chi2_contingency(s3)[0:3]

  #context_type
  ct1 = pd.crosstab(data["skip_1"], data["context_type"])
  skip1_context[file_name] = chi2_contingency(ct1)[0:3]
  ct2 = pd.crosstab(data["skip_2"], data["context_type"])
  skip2_context[file_name] = chi2_contingency(ct2)[0:3]
  ct3 = pd.crosstab(data["skip_3"], data["context_type"])
  skip3_context[file_name] = chi2_contingency(ct3)[0:3]


for file in os.listdir(dir):
  if file.endswith(".csv"):
    find_more_correlations(file, pd.read_csv('/content/training_set/'+file))


#session_length
temp_len = [list(ele) for ele in skip1_length.values()]
mean_skip1_length = sum(i[0] for i in temp_len) / len(skip1_length)
print("mean value between skip1 and session length %.3f" % mean_skip1_length)
p_a = sum(i[1] for i in temp_len) / len(skip1_length)
print("mean p-value %.3f" % p_a)

temp_len2 = [list(ele) for ele in skip2_length.values()]
mean_skip2_length = sum(i[0] for i in temp_len2) / len(skip2_length)
print("mean value between skip2 and session length %.3f" % mean_skip2_length)
p_b = sum(i[1] for i in temp_len2) / len(skip2_length)
print("mean p-value %.3f" % p_b)

temp_len3 = [list(ele) for ele in skip3_length.values()]
mean_skip3_length = sum(i[0] for i in temp_len3) / len(skip3_length)
print("mean value between skip3 and session length %.3f" % mean_skip3_length)
p_c = sum(i[1] for i in temp_len3) / len(skip3_length)
print("mean p-value %.3f" % p_c)

#seekfwd
temp_fwd = [list(ele) for ele in skip1_seekfwd.values()]
mean_skip1_seekfwd = sum(i[0] for i in temp_fwd) / len(skip1_seekfwd)
print("mean value between skip1 and history user behavior seek fwd %.3f" % mean_skip1_seekfwd)
p_d = sum(i[1] for i in temp_fwd) / len(skip1_seekfwd)
print("mean p-value %.3f" % p_d)

temp_fwd2 = [list(ele) for ele in skip2_seekfwd.values()]
mean_skip2_seekfwd = sum(i[0] for i in temp_fwd2) / len(skip2_seekfwd)
print("mean value between skip2 and history user behavior seek fwd %.3f" % mean_skip2_seekfwd)
p_e = sum(i[1] for i in temp_fwd2) / len(skip2_seekfwd)
print("mean p-value %.3f" % p_e)

temp_fwd3 = [list(ele) for ele in skip3_seekfwd.values()]
mean_skip3_seekfwd = sum(i[0] for i in temp_fwd3) / len(skip3_seekfwd)
print("mean value between skip3 and history user behavior seek fwd %.3f" % mean_skip3_seekfwd)
p_f = sum(i[1] for i in temp_fwd3) / len(skip3_seekfwd)
print("mean p-value %.3f" % p_f)

#seekback
temp_back = [list(ele) for ele in skip1_seekback.values()]
mean_skip1_seekback = sum(i[0] for i in temp_back) / len(skip1_seekback)
print("mean value between skip1 and history user behavior of seek back %.3f" % mean_skip1_seekback)
p_g = sum(i[1] for i in temp_back) / len(skip1_seekback)
print("mean p-value %.3f" % p_g)

temp_back2 = [list(ele) for ele in skip2_seekback.values()]
mean_skip2_seekback = sum(i[0] for i in temp_back2) / len(skip2_seekback)
print("mean value between skip2 and history user behavior seek back %.3f" % mean_skip2_seekback)
p_h = sum(i[1] for i in temp_back2) / len(skip2_seekback)
print("mean p-value %.3f" % p_h)

temp_back3 = [list(ele) for ele in skip3_seekback.values()]
mean_skip3_seekback = sum(i[0] for i in temp_back3) / len(skip3_seekback)
print("mean value between skip3 and history user behavior of seek back %.3f" % mean_skip3_seekback)
p_i = sum(i[1] for i in temp_back3) / len(skip3_seekback)
print("mean p-value %.3f" % p_i)

#is_shuffle
temp_shuffle = [list(ele) for ele in skip1_shuffle.values()]
mean_skip1_shuffle = sum(i[0] for i in temp_shuffle) / len(skip1_shuffle)
print("mean value between skip1 and is shuffle %.3f" % mean_skip1_shuffle)
p_j = sum(i[1] for i in temp_shuffle) / len(skip1_shuffle)
print("mean p-value %.3f" % p_j)

temp_shuffle2 = [list(ele) for ele in skip2_shuffle.values()]
mean_skip2_shuffle = sum(i[0] for i in temp_shuffle2) / len(skip2_shuffle)
print("mean value between skip2 and is shuffle %.3f" % mean_skip2_shuffle)
p_k = sum(i[1] for i in temp_shuffle2) / len(skip2_shuffle)
print("mean p-value %.3f" % p_k)

temp_shuffle3 = [list(ele) for ele in skip3_shuffle.values()]
mean_skip3_shuffle = sum(i[0] for i in temp_shuffle3) / len(skip3_shuffle)
print("mean value between skip3 and is shuffle %.3f" % mean_skip3_shuffle)
p_l = sum(i[1] for i in temp_shuffle3) / len(skip3_shuffle)
print("mean p-value %.3f" % p_l)

#context_type
context = [list(ele) for ele in skip1_context.values()]
mean_skip1_context = sum(i[0] for i in context) / len(skip1_context)
print("mean value between skip1 and context type %.3f" % mean_skip1_context)
p_a = sum(i[1] for i in context) / len(skip1_context)
print("mean p-value %.3f" % p_a)

context2 = [list(ele) for ele in skip2_context.values()]
mean_skip2_context = sum(i[0] for i in context2) / len(skip2_context)
print("mean vaue between skip2 and context type %.3f" % mean_skip2_context)
p_b = sum(i[1] for i in context2) / len(skip2_context)
print("mean p-value %.3f" % p_b)

context3 = [list(ele) for ele in skip3_context.values()]
mean_skip3_context = sum(i[0] for i in context3) / len(skip3_context)
print("mean value between skip3 and context type %.3f" % mean_skip3_context)
p_c = sum(i[1] for i in context3) / len(skip3_context)
print("mean p-value %.3f" % p_c)

#this code block aims to find the Chi-Square Measure of Correlation for Context-Type Data

dir = '/content/training_set/'
import pandas as pd 
import os
from scipy.stats import pearsonr

skip1_context, skip2_context, skip3_context = {}, {}, {}

def correlation_context_type(file_name, data):
  (data["skip_1"]).replace([False, True],[0, 1], inplace=True)
  (data["skip_2"]).replace([False, True],[0, 1], inplace=True)
  (data["skip_3"]).replace([False, True],[0, 1], inplace=True)

  (data["context_type"]).replace(["radio", "editorial_playlist", "catalog", "user_collection", "personalized_playlist", "charts"],[0, 1, 2, 3, 4, 5], inplace=True)

  skip1_context[file_name] = pearsonr(data["context_type"], data["skip_1"])
  skip2_context[file_name] = pearsonr(data["context_type"], data["skip_2"])
  skip3_context[file_name] = pearsonr(data["context_type"], data["skip_3"])


for file in os.listdir(dir):
  if file.endswith(".csv"):
    correlation_context_type(file, pd.read_csv('/content/training_set/'+file))

context = [list(ele) for ele in skip1_context.values()]
mean_skip1_context = sum(i[0] for i in context) / len(skip1_context)
print("mean pearson correlation between skip1 and context type %.3f" % mean_skip1_context)
p_a = sum(i[1] for i in context) / len(skip1_context)
print("mean p-value %.3f" % p_a)

context2 = [list(ele) for ele in skip2_context.values()]
mean_skip2_context = sum(i[0] for i in context2) / len(skip2_context)
print("mean pearson correlation between skip2 and context type %.3f" % mean_skip2_context)
p_b = sum(i[1] for i in context2) / len(skip2_context)
print("mean p-value %.3f" % p_b)

context3 = [list(ele) for ele in skip3_context.values()]
mean_skip3_context = sum(i[0] for i in context3) / len(skip3_context)
print("mean pearson correlation between skip3 and context type %.3f" % mean_skip3_context)
p_c = sum(i[1] for i in context3) / len(skip3_context)
print("mean p-value %.3f" % p_c)

dir = '/content/training_set/'
bar1 = {"catalog" : 0, "editorial_playlist" : 0, "radio" : 0, "user_collection" : 0, "personalized_playlist" : 0, "charts":0}
bar2 = {"catalog" : 0, "editorial_playlist" : 0, "radio" : 0, "user_collection" : 0, "personalized_playlist" : 0, "charts":0}
bar3 = {"catalog" : 0, "editorial_playlist" : 0, "radio" : 0, "user_collection" : 0, "personalized_playlist" : 0, "charts":0}
total = {"skip1": 0, "skip2": 0, "skip3": 0}
import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd

#Helper Method to gather the frequency of context_type respective to skip1, skip2, skip3 being positive
def bar_graph(data):
    (data["skip_1"]).replace([False, True],[0, 1], inplace=True)
    (data["skip_2"]).replace([False, True],[0, 1], inplace=True)
    (data["skip_3"]).replace([False, True],[0, 1], inplace=True)
    
    #FREQUENCY OF CONTEXT_TYPE RESPECTIVE TO SKIP_1 IS TRUE
    df_1 = (data.groupby(['skip_1', 'context_type']).size()).reset_index(name='count')
    temp = data.loc[data['skip_1'] == 1]
    total["skip1"] += temp.shape[0]
    bar1["catalog"] += df_1.at[6, 'count'] #catalog frequency
    bar1["charts"] += df_1.at[7, 'count'] #charts frequency
    bar1["editorial_playlist"] += df_1.at[8, 'count'] #editorial frequency 
    bar1["personalized_playlist"] += df_1.at[9, 'count'] #personalized playlist frequency
    bar1["radio"] += df_1.at[10, 'count'] #radio frequency
    bar1["user_collection"] += df_1.at[11, 'count'] #user_collection frequency

    #FREQUENCY OF CONTEXT_TYPE RESPECTIVE TO SKIP_2 IS TRUE
    df_2 = data.groupby(['skip_2', 'context_type']).size().reset_index(name='count')
    df_2.loc[df_2['skip_2'] == 1]
    temp2 = data.loc[data['skip_2'] == 1]
    total["skip2"] += temp2.shape[0]
    bar2["catalog"] += df_2.at[6, 'count'] 
    bar2["charts"] += df_2.at[7, 'count']
    bar2["editorial_playlist"] += df_2.at[8, 'count']
    bar2["personalized_playlist"] += df_2.at[9, 'count']
    bar2["radio"] += df_2.at[10, 'count']
    bar2["user_collection"] += df_2.at[11, 'count']

    #FREQUENCY OF CONTEXT_TYPE RESPECTIVE TO SKIP_3 IS TRUE
    df_3 = data.groupby(['skip_3', 'context_type']).size().reset_index(name='count')
    temp3 = data.loc[data['skip_3'] == 1]
    total["skip3"] += temp3.shape[0]
    bar3["catalog"] += df_3.at[6, 'count']
    bar3["charts"] += df_3.at[7, 'count']
    bar3["editorial_playlist"] += df_3.at[8, 'count']
    bar3["personalized_playlist"] += df_3.at[9, 'count']
    bar3["radio"] += df_3.at[10, 'count']
    bar3["user_collection"] += df_3.at[11, 'count']

for file in os.listdir(dir):
  if file.endswith(".csv"):
    bar_graph(pd.read_csv('/content/training_set/'+file))

print(bar1.keys(), bar1.values())
print(bar2.keys(), bar2.values())
print(bar3.keys(), bar3.values())


f = plt.figure()
f.set_figwidth(15)
f.set_figheight(10)

for key in bar1:    
    bar1[key] = bar1[key] / total["skip1"]

for key in bar2:
    bar2[key] = bar2[key] / total["skip2"]

for key in bar3:
    bar3[key] = bar3[key] / total["skip3"]

df1=pd.DataFrame({'Context Type': bar1.keys(),'Frequency': bar1.values()})
df2=pd.DataFrame({'Context Type': bar2.keys(),'Frequency': bar2.values()})
df3=pd.DataFrame({'Context Type': bar3.keys(),'Frequency': bar3.values()})
print(df1)
print(df2)
print(df3)

plt.title("Frequency of Positive Skip Values Respective to Context_Type")
df1['skip']='skip_very_beginning'
df2['skip']='skip_beginning'
df3['skip'] = 'skip_over_halfway'
res=pd.concat([df1,df2, df3])
sns.barplot(x='Context Type',y='Frequency',data=res,hue='skip')
plt.ylim([0, 100])
plt.show()


f = plt.figure()
f.set_figwidth(15)
f.set_figheight(10)
plt.bar(bar1.keys(), bar1.values())
plt.title("Positive Skip_1 Value Context_Type Breakdown")
plt.xlabel("Context Type")
plt.ylabel("Frequency")
plt.show()


f = plt.figure()
f.set_figwidth(15)
f.set_figheight(10)
plt.bar(bar2.keys(), bar2.values())
plt.title("Positive Skip_2 Value Context_Type Breakdown")
plt.xlabel("Context Type")
plt.ylabel("Frequency")
plt.show()


f = plt.figure()
f.set_figwidth(15)
f.set_figheight(10)
plt.bar(bar3.keys(), bar3.values())
plt.title("Positive Skip_3 Value Context_Type Breakdown")
plt.xlabel("Context Type")
plt.ylabel("Frequency")
plt.show()

#This segment is for graphing the data on a better scale

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
f = plt.figure()
f.set_figwidth(15)
f.set_figheight(10)


bar1 = {"catalog" : 19.5698, "editorial_playlist" : 16.1420, "radio" : 14.9606, "user_collection" : 45.9468, "personalized_playlist" : 1.8654, "charts":1.15155}
bar2 = {"catalog" : 21.0479, "editorial_playlist" : 16.5001, "radio" : 15.1167, "user_collection" : 43.6531, "personalized_playlist" : 2.1625, "charts":1.5196}
bar3 = {"catalog" : 22.23405, "editorial_playlist" : 16.5437, "radio" : 14.8282, "user_collection" : 42.2956, "personalized_playlist" : 2.4979, "charts":1.4941}
df1=pd.DataFrame({'Context Type': bar1.keys(),'Percentage': bar1.values()})
df2=pd.DataFrame({'Context Type': bar2.keys(),'Percentage': bar2.values()})
df3=pd.DataFrame({'Context Type': bar3.keys(),'Percentage': bar3.values()})


plt.title("Frequency of Positive Skip Values Respective to Context_Type")
df1['skip']='skip_very_beginning'
df2['skip']='skip_beginning'
df3['skip'] = 'skip_over_halfway'

res=pd.concat([df1,df2, df3])
sns.barplot(x='Context Type',y='Percentage',data=res,hue='skip')
plt.ylim([0, 50])
plt.show()


df1=pd.DataFrame({'Context type': [1, 2, 3],'y': [5, 10, 15]})
df2=pd.DataFrame({'Context type': [1, 2, 3],'y': [5, 6, 7]})
df3=pd.DataFrame({'Context type': [1, 2, 3],'y': [10, 11, 12]})
df1['skip']='skip_1'
df2['skip']='skip_2'
df3['skip'] = 'skip_3'
res=pd.concat([df1,df2, df3])
sns.barplot(x='Context type',y='y',data=res,hue='skip')
plt.show()

"""Questions I have for our analyses:


*   Which skip value are we correlating variables to (skip_1, skip_2, skip_3) -> I would guess we want to && them and see if at least one of them is true, unless we want to break our analysis into predicting they will skip in skip_1, skip_2, or skip_3 (i.e. what part of the song)
*   examples of variables we can easily correlate: time of day (in hr), session _position (track # in session), premium (have unlimited # of skips), context_type, etc. 
*   Code terribly inefficient right now, which is difficult for testing purposes




"""



# @title Models