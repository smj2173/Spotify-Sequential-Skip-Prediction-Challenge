# -*- coding: utf-8 -*-
"""RandomForestandGradientBoosted.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULFZj0x8RyRB9x3xzqiZs5IqdpuMTlVc
"""

# @title Download Training Set and untar
#runtime about 3 minutes
!wget https://os.zhdk.cloud.switch.ch/swift/v1/crowdai-public/spotify-sequential-skip-prediction-challenge/split-files/training_set_0.tar.gz

!tar -xvf 'training_set_0.tar.gz'
#Untars the dataset 
#runtime about 4 minutes

# @title Convert into dataframe and merge skip columns 
import os
import pandas as pd
import glob
import pickle

# merge the skip columns into one: based on skip_3 values 
# is skip_3 is true -> skip is true
# if skip_3 is false -> skip is false
# merging the files
joined_files = os.path.join("./training_set", "log_0_*.csv")
  
# A list of all joined files is returned
joined_list = glob.glob(joined_files)

# Finally, the files are joined
data = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)
#data = pd.concat(map(pd.read_csv, glob.glob(os.path.join("./training_set", "log_0_*.csv"))), ignore_index= True)

# @title Merge CSVs
CHUNK_SIZE = 50000
csv_file_list = ["/content/training_set/log_0_20180715_000000000000.csv", "/content/training_set/log_0_20180716_000000000000.csv", "/content/training_set/log_0_20180717_000000000000.csv", "/content/training_set/log_0_20180718_000000000000.csv", "/content/training_set/log_0_20180719_000000000000.csv", "./training_set/log_0_20180720_000000000000.csv","./training_set/log_0_20180721_000000000000.csv",
"./training_set/log_0_20180722_000000000000.csv",
"./training_set/log_0_20180723_000000000000.csv",
"./training_set/log_0_20180724_000000000000.csv",
"./training_set/log_0_20180725_000000000000.csv",
"./training_set/log_0_20180726_000000000000.csv",
"./training_set/log_0_20180727_000000000000.csv",
"./training_set/log_0_20180728_000000000000.csv",
"./training_set/log_0_20180729_000000000000.csv",
"./training_set/log_0_20180730_000000000000.csv",
"./training_set/log_0_20180731_000000000000.csv",
"./training_set/log_0_20180801_000000000000.csv",
"./training_set/log_0_20180802_000000000000.csv",
"./training_set/log_0_20180803_000000000000.csv",
"./training_set/log_0_20180804_000000000000.csv",
"./training_set/log_0_20180805_000000000000.csv",
"./training_set/log_0_20180806_000000000000.csv",
"./training_set/log_0_20180807_000000000000.csv",
"./training_set/log_0_20180808_000000000000.csv",
"./training_set/log_0_20180809_000000000000.csv",
"./training_set/log_0_20180810_000000000000.csv",
"./training_set/log_0_20180811_000000000000.csv",
"./training_set/log_0_20180812_000000000000.csv",
"./training_set/log_0_20180813_000000000000.csv",
"./training_set/log_0_20180814_000000000000.csv",
"./training_set/log_0_20180815_000000000000.csv",
"./training_set/log_0_20180816_000000000000.csv",
"./training_set/log_0_20180817_000000000000.csv",
"./training_set/log_0_20180818_000000000000.csv",
"./training_set/log_0_20180819_000000000000.csv",
"./training_set/log_0_20180820_000000000000.csv",
"./training_set/log_0_20180821_000000000000.csv",
"./training_set/log_0_20180822_000000000000.csv",
"./training_set/log_0_20180823_000000000000.csv",
"./training_set/log_0_20180824_000000000000.csv",
"./training_set/log_0_20180825_000000000000.csv",
"./training_set/log_0_20180826_000000000000.csv",
"./training_set/log_0_20180827_000000000000.csv",
"./training_set/log_0_20180828_000000000000.csv",
"./training_set/log_0_20180829_000000000000.csv",
"./training_set/log_0_20180830_000000000000.csv",
"./training_set/log_0_20180831_000000000000.csv",
"./training_set/log_0_20180901_000000000000.csv",
"./training_set/log_0_20180902_000000000000.csv",
"./training_set/log_0_20180903_000000000000.csv",
"./training_set/log_0_20180904_000000000000.csv",
"./training_set/log_0_20180905_000000000000.csv",
"./training_set/log_0_20180906_000000000000.csv",
"./training_set/log_0_20180907_000000000000.csv",
"./training_set/log_0_20180908_000000000000.csv",
"./training_set/log_0_20180909_000000000000.csv",
"./training_set/log_0_20180910_000000000000.csv",
"./training_set/log_0_20180911_000000000000.csv",
"./training_set/log_0_20180912_000000000000.csv",
"./training_set/log_0_20180913_000000000000.csv",
"./training_set/log_0_20180914_000000000000.csv",
"./training_set/log_0_20180915_000000000000.csv",
"./training_set/log_0_20180916_000000000000.csv",
"./training_set/log_0_20180917_000000000000.csv",
"./training_set/log_0_20180918_000000000000.csv"]
output_file = "final_output.csv"

CHUNK_SIZE = 50000

first_one = True
for csv_file_name in csv_file_list:

    if not first_one: # if it is not the first csv file then skip the header row (row 0) of that file
        skip_row = [0]
    else:
        skip_row = []

    chunk_container = pd.read_csv(csv_file_name, chunksize=CHUNK_SIZE, skiprows = skip_row)
    for chunk in chunk_container:
        chunk.to_csv(output_file, mode="a", header=False, index=False)
    first_one = False

# @title Convert into dataframe and merge skip columns
output_file = "output.csv"
data = pd.read_csv(output_file, names=["session_id", "session_position", "session_length", "track_id_clean", "skip_1", "skip_2", "skip_3", "not_skipped", "context_switch", "no_pause_before_play", "short_pause_before_play", "long_pause_before_play", "hist_user_behavior_n_seekfwd", "hist_user_behavior_n_seekback", "hist_user_behavior_is_shuffle", "hour_of_day", "date", "premium", "context_type", "hist_user_behavior_reason_start", "hist_user_behavior_reason_end"], low_memory=False)
data

# aggregate skip columns: based on skip_3 values 
# is skip_3 is true -> skip is true
# if skip_3 is false -> skip is false

data = data[~data["not_skipped"].isnull()] 
data["skipped"] = data["skip_1"] | data["skip_2"] | data["skip_3"]
# normalize columns so that they are float vlaues 
data["skipped"] = data["skipped"].astype(int)
data['not_skipped'] = data['not_skipped'].astype(int)
data["session_length"] = (data["session_length"].astype(int)) / 20
data['hist_user_behavior_is_shuffle'] = data['hist_user_behavior_is_shuffle'].astype(int)
del data['skip_1']
del data['skip_2']
del data['skip_3']

for col in data.columns:
    print(col)

data.head()

# @title Selecting the features to extract
# skip_1: context_type, session_length, hist_user_behavior_n_seekback, hist_user_behavior_is_shuffle, context_switch, no_pause_before_play, session_position, long_pause_before_play
# skip_2: session_length, hist_user_behavior_n_seekfwd, hist_user_behavior_is_shuffle, context_switch, no_pause_before_play, long_pause_before_play
# skip_3: session_length, hist_user_behavior_n_seekfwd, no_pause_before_play, long_pause_before_play

# overall skip: context_type, session_length, hist_user_behavior_n_seekback, hist_user_behavior_n_seekfwd, hist_user_behavior_is_shuffle, context_switch, no_pause_before_play, session_position, long_pause_before_play
# extract features from skip_1, skip_2, and skip_3
# feed into classifier the training set values of the extracted features for skip_2 and skip_3 and not_skipped
# then, use random forest classifier to predict whether a given session is part of which 2 categories (skip_1/skip_2/skip_3, or not_skipped)
# evaluate based on how accurate it was in predicting for the test set
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.model_selection import train_test_split

x = data[['session_length', 'hist_user_behavior_n_seekback', 'hist_user_behavior_n_seekfwd', 'hist_user_behavior_is_shuffle', 'context_switch', 'no_pause_before_play', 'session_position', 'long_pause_before_play']]  # Features
y = data['skipped']  # Labels

# @title Define train and test variables

# Split dataset into training set and test set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)

clf=RandomForestClassifier(n_estimators=100,max_depth=5,verbose=2)

#Train the model
clf.fit(x_train,y_train)

y_pred=clf.predict(x_test)

from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print(metrics.confusion_matrix(y_test, y_pred))
print(metrics.classification_report(y_test,y_pred))

# @title Exploratory Data Analysis: Feature Importance
feature_names = ['session_length', 'hist_user_behavior_n_seekback', 'hist_user_behavior_n_seekfwd', 'hist_user_behavior_is_shuffle', 'context_switch', 'no_pause_before_play', 'session_position', 'long_pause_before_play']
feature_imp = pd.Series(clf.feature_importances_,index=feature_names).sort_values(ascending=False)
feature_imp

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.show()

# @title Gradient Boosted Tree
import numpy as np
from sklearn.model_selection import KFold
from sklearn.ensemble import GradientBoostingClassifier

x = data[['session_length', 'hist_user_behavior_n_seekback', 'hist_user_behavior_n_seekfwd', 'hist_user_behavior_is_shuffle', 'context_switch', 'no_pause_before_play', 'session_position', 'long_pause_before_play']]  # Features
y = data['skipped']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)

#kf = KFold(n_splits=5,random_state=42,shuffle=True)
#for train_index,val_index in kf.split(x):
 #   x_train, x_test = x.iloc[train_index],x.iloc[val_index],
 #   y_train,y_test = y.iloc[train_index],y.iloc[val_index],

gradient_booster = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=10, verbose=1)

gradient_booster.fit(x_train,y_train)
y_pred=gradient_booster.predict(x_test)
print(metrics.classification_report(y_test,y_pred))
print(metrics.confusion_matrix(y_test,y_pred))

